{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\multiconstraint\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2057e6c6430>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# DPO（Direct Preference Optimization）算法实现\n",
    "# DPO通过人类偏好数据直接优化语言模型，使其生成更符合人类偏好的输出\n",
    "# 这里面使用了一个偏好prefer以及两个reject的格式\n",
    "# ===============================================================================\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaForCausalLM, LlamaConfig\n",
    "from copy import deepcopy\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(12, 32)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (k_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (v_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (o_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=32, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=32, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=32, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((32,), eps=1e-06)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((32,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((32,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=32, out_features=12, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型\n",
    "# 创建简化版的Llama模型作为策略模型（将被优化的模型）\n",
    "policy_model = LlamaForCausalLM(config=LlamaConfig(vocab_size=12, num_hidden_layers=1, hidden_size=32))\n",
    "# 创建参考模型（通常是SFT模型，在训练过程中保持不变）\n",
    "reference_model = deepcopy(policy_model)  # 深度复制确保两个模型初始参数完全相同\n",
    "policy_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "beta = 0.1  # DPO的温度系数，控制策略模型与参考模型的偏离程度，值越小允许偏离越大\n",
    "\n",
    "# 准备训练数据\n",
    "# 在DPO中，我们需要提示(prompt)、优选回答(chosen/good)和拒绝回答(rejected/bad)\n",
    "prompt_ids = [1, 2, 3, 4, 5, 6]  # 输入提示的token IDs\n",
    "good_response_ids = [7, 8, 9, 2]  # 优质回答的token IDs\n",
    "# 多个低质量回答的示例，每个都是token IDs的列表\n",
    "bad_response_ids_list = [[1, 2, 0, 0], [4, 5, 6, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 2],\n",
       "        [1, 2, 3, 4, 5, 6, 1, 2, 0, 0],\n",
       "        [1, 2, 3, 4, 5, 6, 4, 5, 6, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建模型输入：将提示与回答拼接\n",
    "# 创建包含多个序列的批次：[提示+优质回答, 提示+低质回答1, 提示+低质回答2, ...]\n",
    "input_ids = torch.LongTensor(\n",
    "    [prompt_ids + good_response_ids, *[prompt_ids + bad_response_ids for bad_response_ids in bad_response_ids_list]]\n",
    ")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100, -100,    7,    8,    9,    2],\n",
       "        [-100, -100, -100, -100, -100, -100,    1,    2,    0,    0],\n",
       "        [-100, -100, -100, -100, -100, -100,    4,    5,    6,    0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备用于计算语言模型损失的标签\n",
    "# 在语言模型训练中，标签是输入向右移动一位（预测下一个token）\n",
    "# -100表示在计算损失时忽略该位置（这里忽略提示部分）\n",
    "labels = torch.LongTensor(\n",
    "    [\n",
    "        [-100] * len(prompt_ids) + good_response_ids,\n",
    "        *[[-100] * len(prompt_ids) + bad_response_ids for bad_response_ids in bad_response_ids_list]\n",
    "    ]\n",
    ")# 向右移动一位，因为我们预测的是下一个token\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100,    7,    8,    9,    2],\n",
       "        [-100, -100, -100, -100, -100,    1,    2,    0,    0],\n",
       "        [-100, -100, -100, -100, -100,    4,    5,    6,    0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = labels[:, 1:]  \n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建掩码，用于标识哪些位置参与损失计算（即回答部分）\n",
    "loss_mask = (labels != -100)\n",
    "print(loss_mask.shape)\n",
    "loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 7, 8, 9, 2],\n",
       "        [0, 0, 0, 0, 0, 1, 2, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 4, 5, 6, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将-100替换为0，因为在gather操作中-100是无效索引\n",
    "labels[labels == -100] = 0\n",
    "print(labels.shape)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: torch.Size([3, 10, 12])\n",
      "past_key_values: <class 'transformers.cache_utils.DynamicCache'>\n"
     ]
    }
   ],
   "source": [
    "output = policy_model(input_ids)\n",
    "for key, value in output.items():  # 如果是ModelOutput对象，可以用output.__dict__.items()\n",
    "    if hasattr(value, \"shape\"):\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: {type(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0565, -0.0827, -0.1474,  0.0100, -0.0097, -0.1095,  0.0679,\n",
       "           0.0035, -0.0819, -0.0010],\n",
       "         [ 0.1082, -0.1835, -0.1182, -0.1179,  0.0160, -0.1471,  0.0077,\n",
       "          -0.1568, -0.0925,  0.0218],\n",
       "         [ 0.0287, -0.1253,  0.0064, -0.0516, -0.0354, -0.0353, -0.0429,\n",
       "          -0.2031, -0.0248, -0.0239],\n",
       "         [-0.0219,  0.1550,  0.0996,  0.1397, -0.1033,  0.0055, -0.1241,\n",
       "           0.0612,  0.0729, -0.0935],\n",
       "         [-0.1054,  0.0074, -0.1446, -0.1313, -0.0460,  0.0428,  0.0211,\n",
       "           0.0946,  0.1309, -0.0070],\n",
       "         [ 0.0928, -0.1962, -0.0238, -0.0858, -0.1913, -0.2821, -0.0004,\n",
       "          -0.1976,  0.0196, -0.1160],\n",
       "         [-0.1436,  0.0744, -0.0164,  0.0534,  0.0762, -0.0857, -0.2603,\n",
       "          -0.0457,  0.1537,  0.0267],\n",
       "         [-0.1182,  0.0489, -0.0797,  0.0039, -0.0279,  0.1024,  0.0110,\n",
       "           0.0005, -0.0170, -0.0856],\n",
       "         [-0.0509, -0.0215, -0.0647,  0.0435, -0.0992,  0.0381, -0.1139,\n",
       "           0.0671, -0.0092, -0.1155]],\n",
       "\n",
       "        [[-0.0565, -0.0827, -0.1474,  0.0100, -0.0097, -0.1095,  0.0679,\n",
       "           0.0035, -0.0819, -0.0010],\n",
       "         [ 0.1082, -0.1835, -0.1182, -0.1179,  0.0160, -0.1471,  0.0077,\n",
       "          -0.1568, -0.0925,  0.0218],\n",
       "         [ 0.0287, -0.1253,  0.0064, -0.0516, -0.0354, -0.0353, -0.0429,\n",
       "          -0.2031, -0.0248, -0.0239],\n",
       "         [-0.0219,  0.1550,  0.0996,  0.1397, -0.1033,  0.0055, -0.1241,\n",
       "           0.0612,  0.0729, -0.0935],\n",
       "         [-0.1054,  0.0074, -0.1446, -0.1313, -0.0460,  0.0428,  0.0211,\n",
       "           0.0946,  0.1309, -0.0070],\n",
       "         [ 0.0928, -0.1962, -0.0238, -0.0858, -0.1913, -0.2821, -0.0004,\n",
       "          -0.1976,  0.0196, -0.1160],\n",
       "         [-0.0445, -0.0359, -0.1230, -0.0629, -0.0283, -0.1006,  0.0080,\n",
       "           0.0279, -0.1417,  0.0006],\n",
       "         [ 0.0762, -0.1421, -0.0977, -0.1530, -0.0307, -0.1572, -0.0045,\n",
       "          -0.1844, -0.1458,  0.0421],\n",
       "         [ 0.1235, -0.1921,  0.1505,  0.1238,  0.0671, -0.1188,  0.0918,\n",
       "          -0.1072,  0.0289, -0.0068]],\n",
       "\n",
       "        [[-0.0565, -0.0827, -0.1474,  0.0100, -0.0097, -0.1095,  0.0679,\n",
       "           0.0035, -0.0819, -0.0010],\n",
       "         [ 0.1082, -0.1835, -0.1182, -0.1179,  0.0160, -0.1471,  0.0077,\n",
       "          -0.1568, -0.0925,  0.0218],\n",
       "         [ 0.0287, -0.1253,  0.0064, -0.0516, -0.0354, -0.0353, -0.0429,\n",
       "          -0.2031, -0.0248, -0.0239],\n",
       "         [-0.0219,  0.1550,  0.0996,  0.1397, -0.1033,  0.0055, -0.1241,\n",
       "           0.0612,  0.0729, -0.0935],\n",
       "         [-0.1054,  0.0074, -0.1446, -0.1313, -0.0460,  0.0428,  0.0211,\n",
       "           0.0946,  0.1309, -0.0070],\n",
       "         [ 0.0928, -0.1962, -0.0238, -0.0858, -0.1913, -0.2821, -0.0004,\n",
       "          -0.1976,  0.0196, -0.1160],\n",
       "         [-0.0497,  0.1791,  0.0838,  0.1562, -0.0954,  0.0160, -0.0987,\n",
       "           0.0749,  0.0925, -0.0861],\n",
       "         [-0.1003, -0.0116, -0.1442, -0.1418, -0.0371,  0.0671,  0.0496,\n",
       "           0.0896,  0.1160,  0.0103],\n",
       "         [ 0.0825, -0.1899, -0.0342, -0.0874, -0.1939, -0.2715,  0.0080,\n",
       "          -0.1895,  0.0141, -0.1027]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# 计算策略模型（policy model）的对数概率\n",
    "# ===============================================================================\n",
    "# 前向传播，获取每个token位置的预测logits\n",
    "logits = policy_model(input_ids)[\"logits\"][:, :-1, :]  # 去掉最后一个位置，与label对齐\n",
    "print(logits.shape)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.7857, -6.6947, -6.7697, -6.5169, -6.6811, -7.0314, -6.7662, -6.5559,\n",
       "         -6.6359],\n",
       "        [-6.7857, -6.6947, -6.7697, -6.5169, -6.6811, -7.1295, -6.9317, -6.6988,\n",
       "         -6.4697],\n",
       "        [-6.7857, -6.6947, -6.7697, -6.5169, -6.6811, -6.7405, -6.8247, -7.2198,\n",
       "         -6.8045]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将logits转换为对数概率，并提取每个位置上正确token的对数概率\n",
    "per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n",
    "per_token_logps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-60.4376, -60.6779, -61.0376], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 仅对回答部分（loss_mask=True的位置）求和，得到每个序列的总对数概率\n",
    "all_logps = (per_token_logps * loss_mask).sum(-1)\n",
    "all_logps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分离优质回答和低质量回答的对数概率\n",
    "policy_good_logps, policy_bad_logps = all_logps[:1], all_logps[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-60.4376], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_good_logps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-60.6779, -61.0376], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_bad_logps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9, 10])\n",
      "logits:\n",
      " tensor([[[-0.0565, -0.0827, -0.1474,  0.0100, -0.0097, -0.1095,  0.0679,\n",
      "           0.0035, -0.0819, -0.0010],\n",
      "         [ 0.1082, -0.1835, -0.1182, -0.1179,  0.0160, -0.1471,  0.0077,\n",
      "          -0.1568, -0.0925,  0.0218],\n",
      "         [ 0.0287, -0.1253,  0.0064, -0.0516, -0.0354, -0.0353, -0.0429,\n",
      "          -0.2031, -0.0248, -0.0239],\n",
      "         [-0.0219,  0.1550,  0.0996,  0.1397, -0.1033,  0.0055, -0.1241,\n",
      "           0.0612,  0.0729, -0.0935],\n",
      "         [-0.1054,  0.0074, -0.1446, -0.1313, -0.0460,  0.0428,  0.0211,\n",
      "           0.0946,  0.1309, -0.0070],\n",
      "         [ 0.0928, -0.1962, -0.0238, -0.0858, -0.1913, -0.2821, -0.0004,\n",
      "          -0.1976,  0.0196, -0.1160],\n",
      "         [-0.1436,  0.0744, -0.0164,  0.0534,  0.0762, -0.0857, -0.2603,\n",
      "          -0.0457,  0.1537,  0.0267],\n",
      "         [-0.1182,  0.0489, -0.0797,  0.0039, -0.0279,  0.1024,  0.0110,\n",
      "           0.0005, -0.0170, -0.0856],\n",
      "         [-0.0509, -0.0215, -0.0647,  0.0435, -0.0992,  0.0381, -0.1139,\n",
      "           0.0671, -0.0092, -0.1155]],\n",
      "\n",
      "        [[-0.0565, -0.0827, -0.1474,  0.0100, -0.0097, -0.1095,  0.0679,\n",
      "           0.0035, -0.0819, -0.0010],\n",
      "         [ 0.1082, -0.1835, -0.1182, -0.1179,  0.0160, -0.1471,  0.0077,\n",
      "          -0.1568, -0.0925,  0.0218],\n",
      "         [ 0.0287, -0.1253,  0.0064, -0.0516, -0.0354, -0.0353, -0.0429,\n",
      "          -0.2031, -0.0248, -0.0239],\n",
      "         [-0.0219,  0.1550,  0.0996,  0.1397, -0.1033,  0.0055, -0.1241,\n",
      "           0.0612,  0.0729, -0.0935],\n",
      "         [-0.1054,  0.0074, -0.1446, -0.1313, -0.0460,  0.0428,  0.0211,\n",
      "           0.0946,  0.1309, -0.0070],\n",
      "         [ 0.0928, -0.1962, -0.0238, -0.0858, -0.1913, -0.2821, -0.0004,\n",
      "          -0.1976,  0.0196, -0.1160],\n",
      "         [-0.0445, -0.0359, -0.1230, -0.0629, -0.0283, -0.1006,  0.0080,\n",
      "           0.0279, -0.1417,  0.0006],\n",
      "         [ 0.0762, -0.1421, -0.0977, -0.1530, -0.0307, -0.1572, -0.0045,\n",
      "          -0.1844, -0.1458,  0.0421],\n",
      "         [ 0.1235, -0.1921,  0.1505,  0.1238,  0.0671, -0.1188,  0.0918,\n",
      "          -0.1072,  0.0289, -0.0068]],\n",
      "\n",
      "        [[-0.0565, -0.0827, -0.1474,  0.0100, -0.0097, -0.1095,  0.0679,\n",
      "           0.0035, -0.0819, -0.0010],\n",
      "         [ 0.1082, -0.1835, -0.1182, -0.1179,  0.0160, -0.1471,  0.0077,\n",
      "          -0.1568, -0.0925,  0.0218],\n",
      "         [ 0.0287, -0.1253,  0.0064, -0.0516, -0.0354, -0.0353, -0.0429,\n",
      "          -0.2031, -0.0248, -0.0239],\n",
      "         [-0.0219,  0.1550,  0.0996,  0.1397, -0.1033,  0.0055, -0.1241,\n",
      "           0.0612,  0.0729, -0.0935],\n",
      "         [-0.1054,  0.0074, -0.1446, -0.1313, -0.0460,  0.0428,  0.0211,\n",
      "           0.0946,  0.1309, -0.0070],\n",
      "         [ 0.0928, -0.1962, -0.0238, -0.0858, -0.1913, -0.2821, -0.0004,\n",
      "          -0.1976,  0.0196, -0.1160],\n",
      "         [-0.0497,  0.1791,  0.0838,  0.1562, -0.0954,  0.0160, -0.0987,\n",
      "           0.0749,  0.0925, -0.0861],\n",
      "         [-0.1003, -0.0116, -0.1442, -0.1418, -0.0371,  0.0671,  0.0496,\n",
      "           0.0896,  0.1160,  0.0103],\n",
      "         [ 0.0825, -0.1899, -0.0342, -0.0874, -0.1939, -0.2715,  0.0080,\n",
      "          -0.1895,  0.0141, -0.1027]]])\n",
      "torch.Size([3, 9])\n",
      "per_token_logps:\n",
      " tensor([[-2.3203, -2.1325, -2.2251, -2.3483, -2.3982, -2.4085, -2.1387, -2.3740,\n",
      "         -2.3367],\n",
      "        [-2.3203, -2.1325, -2.2251, -2.3483, -2.3982, -2.4071, -2.3770, -2.1506,\n",
      "         -2.2013],\n",
      "        [-2.3203, -2.1325, -2.2251, -2.3483, -2.3982, -2.4022, -2.3188, -2.2468,\n",
      "         -2.1295]])\n",
      "torch.Size([3])\n",
      "all_logps\n",
      " tensor([-9.2579, -9.1361, -9.0972])\n",
      "torch.Size([1])\n",
      "reference_good_logps:\n",
      " tensor([-9.2579])\n",
      "torch.Size([2])\n",
      "reference_bad_logps\n",
      " tensor([-9.1361, -9.0972])\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# 计算参考模型（reference model）的对数概率\n",
    "# ===============================================================================\n",
    "with torch.no_grad():  # 不计算梯度，因为参考模型不需要更新\n",
    "    # 重复与策略模型相同的步骤\n",
    "    logits = reference_model(input_ids)[\"logits\"][:, :-1, :]\n",
    "    print(logits.shape)\n",
    "    print(\"logits:\\n\",logits)\n",
    "    per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n",
    "    print(per_token_logps.shape)\n",
    "    print(\"per_token_logps:\\n\",per_token_logps)\n",
    "    all_logps = (per_token_logps * loss_mask).sum(-1)\n",
    "    print(all_logps.shape)\n",
    "    print(\"all_logps\\n\",all_logps)\n",
    "    reference_good_logps, reference_bad_logps = all_logps[:1], all_logps[1:]\n",
    "    print(reference_good_logps.shape)\n",
    "    print(\"reference_good_logps:\\n\",reference_good_logps)\n",
    "    print(reference_bad_logps.shape)\n",
    "    print(\"reference_bad_logps\\n\",reference_bad_logps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# 计算DPO损失\n",
    "# DPO的核心思想：增大策略模型对优质回答的概率，同时减小对低质量回答的概率\n",
    "# ===============================================================================\n",
    "# 计算DPO的logits：(策略模型相对于参考模型对好回答的提升) - (对坏回答的提升)\n",
    "logits = (policy_good_logps - reference_good_logps) - (policy_bad_logps - reference_bad_logps)\n",
    "# 应用logsigmoid函数并乘以beta控制优化强度，取负值（因为要最小化损失）\n",
    "loss = -F.logsigmoid(beta * logits).mean()  # 对所有样本取平均\n",
    "\n",
    "# 输出损失值\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiconstraint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
