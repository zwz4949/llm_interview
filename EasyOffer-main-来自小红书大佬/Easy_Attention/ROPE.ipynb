{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        \n",
    "        # 构建旋转频率\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        \n",
    "        # 预计算缓存以加速推理\n",
    "        self._set_cos_sin_cache(max_position_embeddings)\n",
    "        \n",
    "    def _set_cos_sin_cache(self, seq_len):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        \n",
    "        # 计算不同位置的频率\n",
    "        t = torch.arange(seq_len, device=self.inv_freq.device).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        \n",
    "        # 计算旋转角度的正弦和余弦值\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        cos = emb.cos()\n",
    "        sin = emb.sin()\n",
    "        self.register_buffer(\"cos_cached\", cos.float())\n",
    "        self.register_buffer(\"sin_cached\", sin.float())\n",
    "        \n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: [batch_size, seq_len, num_heads, head_dim]\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len)\n",
    "            \n",
    "        return (\n",
    "            self.cos_cached[:seq_len].to(x.device),\n",
    "            self.sin_cached[:seq_len].to(x.device)\n",
    "        )\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n",
    "    # q, k: [batch_size, seq_len, num_heads, head_dim]\n",
    "    # cos, sin: [seq_len, head_dim]\n",
    "    # position_ids: [batch_size, seq_len]\n",
    "    \n",
    "    # 获取q和k的形状信息\n",
    "    batch_size, seq_length, num_heads, head_dim = q.shape\n",
    "    \n",
    "    # 根据position_ids获取对应位置的cos和sin\n",
    "    cos = cos.index_select(0, position_ids.reshape(-1)).reshape(batch_size, seq_length, 1, head_dim)\n",
    "    sin = sin.index_select(0, position_ids.reshape(-1)).reshape(batch_size, seq_length, 1, head_dim)\n",
    "    print(cos.shape)\n",
    "    # 将head_dim维度拆分为两部分\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    \n",
    "    return q_embed, k_embed\n",
    "\n",
    "def rotate_half(x):\n",
    "    # 旋转向量的一半维度\n",
    "    x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "class RopeEmbed(nn.Module):\n",
    "    def __init__(self, dim, base=10000):\n",
    "        super().__init__()\n",
    "        ## xita每二个建一个,后面拼起来，因为每2个维度下标对应一个xita，m每个token建一个\n",
    "        xita = 1.0/(base ** (2 * torch.arange(0, dim, 2) /dim))\n",
    "        # print(xita.shape)\n",
    "        self.register_buffer(\"xita\", xita)\n",
    "\n",
    "    def forward(self,hidden_states):\n",
    "        # hidden_states [bs, seqlen, dim]\n",
    "        seq_len = hidden_states.shape[1]\n",
    "        bs = hidden_states.shape[0]\n",
    "        xita_expanded = self.xita[None,:,None].expand(hidden_states.shape[0],-1,1)\n",
    "        # xita_expanded = self.xita.expand()\n",
    "        position_indexes_expanded = torch.arange(seq_len)[None,None,:].expand(bs, 1, -1)\n",
    "        freqs = (xita_expanded.float() @ position_indexes_expanded.float()).transpose(1,2)\n",
    "        # print(freqs.shape)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        emb_cos = emb.cos()\n",
    "        emb_sin = emb.sin()\n",
    "        return emb_cos, emb_sin\n",
    "\n",
    "def rotate(hidden_states):\n",
    "    x1 ,x2 = hidden_states[..., :hidden_states.shape[-1]//2], hidden_states[..., hidden_states.shape[-1]//2:]\n",
    "    return torch.cat((-x2,x1),dim=-1)\n",
    "def apply_rotary_pos_emb(q,k,cos,sin):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    ## q [bs, seqlen, head,headdim]\n",
    "    ## k [bs, seqlen, head,headdim]\n",
    "    ## cos,sin [bs, seqlen, headdim]\n",
    "    cos = cos.unsqueeze(2)\n",
    "    sin = sin.unsqueeze(2)\n",
    "    q_rot = (q * cos) + (rotate(q) * sin)\n",
    "    k_rot = (k * cos) + (rotate(k) * sin)\n",
    "    return q_rot, k_rot\n",
    "ropeemb = RopeEmbed(128)\n",
    "cos,sin = ropeemb(torch.randn(2,128,128))\n",
    "q,k = torch.randn(2,128,8,128),torch.randn(2,128,8,128)\n",
    "apply_rotary_pos_emb(q,k,cos,sin)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
