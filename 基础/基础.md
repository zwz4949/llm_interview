> 怎么解决过拟合问题
过拟合指的是模型在训练集上表现优秀，但在测试集或新数据上表现较差，无法很好地泛化。
过拟合通常源于模型过于复杂或数据不足。
解决方法：
1. 增加训练数据，更多的数据能帮助模型学习数据的真实分布，减少对噪声或局部特征的依赖
2. Dropout，训练时随机“关闭”一部分神经元
3. 降低模型复杂度，避免过度拟合训练数据中的噪声
4. 正则化，在损失函数中加入惩罚项，限制参数值过大，从而降低模型复杂度
5. LoRA 通过低秩分解（Low-Rank Decomposition）来更新权重矩阵，仅训练少量参数（如低秩矩阵），而非全参数微调，从而降低模型复杂度，减少过拟合风险（同3）

> 反向传播过程

前向传播完后，得到模型的输出，然后通过与label对比计算得到loss，接着对损失求导并通过链式法则（chain rule）计算各层的参数梯度。

<反向传播过程指的是利用链式法则高效地“计算梯度”的过程，而**不包括**使用这些梯度更新参数的步骤>

> 优化器更新过程

用反向传播计算的这些梯度更新参数，然后根据模型参数梯度更新模型参数。

![alt text](image.png)
![alt text](image-1.png)


> 为什么decoder-only，而不用encoder-decoder

从kvcache的角度讲：

encoder-decoder的encoder是双向注意力，每个query要和所有的key进行注意力计算，计算效率差，速度慢；decoder-only的decoder是单向注意力，每个query只和前面的key进行注意力计算，计算效率高，速度快。

在推理时每生成一个token，encoder的话当前query以及之前的query的token都要和之前以及当前的k计算，decoder只有当前的query要和之前的和当前的k计算。

> kv cache

在自回归（autoregressive）生成时，每一步都会计算一次 Attention，传统做法需要针对整个上下文序列和和已生成的token重新投影得到所有的 Key（K）和 Value（V），计算复杂度随序列长度平方增长。
KV Cache 就是在解码过程中，把前面各层已计算出的 K、V 张量缓存在显存中，后续生成新 token 时只需对新 token 计算一次 Q（Query），再与缓存的 K、V 做 Attention，就能得到输出，大幅降低重复计算。
