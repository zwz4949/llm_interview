#### GPT类的模型结构，训练目标
decoder-only 没有cross attention，训练时任务的输入和任务的输出拼在一起为模型输入，一起过self attetnion，qkv都是这个输入

#### 能不能画一下transformer的结构？

#### 介绍下transformer的结构；现在主流的大模型架构和transformer有什么改进
改进：
1. 把归一化改成在残差连接前，有助于梯度传播，
2. layernorm改成rmsnorm，去掉期望值的计算，减轻计算负担，同时效果变化不大
3. 使用旋转位置编码替换原始transformers得绝对位置编码，rope是基于绝对位置编码的相对位置编码，给q和k分别乘上一个旋转矩阵，求两个token的相对位置时就相当于旋转矩阵中的三角函数的角度前的下标相减
#### 计算注意力 为什么除以根号dk
softmax的输入变大，会出现梯度极小的情况，导致梯度消失

#### 为什么用layernorm 不用batchnorm
文本这边的话，如果用batchnorm，对同个batch的每个特征的同个位置进行归一化，但是这些位置之间可能并不像图像那样有关系，所以效果不好；layernorm是对单个特征进行归一化
#### RMSNorm相较于LayerNorm有什么优势？公式是什么？
减少了期望的计算，x * p/sqrt((1/n) * x^2 + xita)
#### Self-Attention计算复杂度多少？
1. Q/K/V 投影：每个 token 需与 3 个 d×d 的权重矩阵相乘，总复杂度 O (nd²)，属于低阶项。
2. 注意力分数计算：Q（n×d）与 K^T（d×n）矩阵相乘，得到 n×n 的注意力矩阵，复杂度 O (n²d)，是主导项。
3. 分数与 V 加权：n×n 的注意力矩阵与 V（n×d）相乘，复杂度 O (n²d)，同样是主导项。
4. 输出投影：最终结果与 d×d 权重矩阵相乘，复杂度 O (nd²)，为低阶项。
#### 解释下交叉注意力机制
有encoder的时候有交叉注意力，他接受encoder的输出做为k和v，接受decoder中 self attention的输出作为q
#### 介绍mha、gqa、mqa、mla
mha是多头注意力机制，每个q头都对应一个kv头；gqa是分组多头注意力，每n个q头为一组，一组对应一个kv头（k和v的dim为dim/n）；mqa是所有q头对应一个kv头（k和v的dim不变）；mla如下所示

#### 讲一下MLA，除了低秩分解MLA还做了什么;mla 都哪些矩阵做压缩
将注意力机制中的键（Key）和值（Value）矩阵投影到低维潜在空间，这种压缩通过双线性变换实现：


   $$c_t^{KV} = W^{DKV} \cdot h_t$$

   其中，$$h_t$$为输入特征，$$W^{DKV}$$为压缩矩阵，$$c_t^{KV}$$为潜在向量。在推理时，通过上投影矩阵将潜在向量恢复为原始维度，从而减少显存占用并加速计算。
#### 多头注意力和单头注意力计算时间复杂度一致吗，时间复杂度是多少？
一样，都是n方d

#### 大模型位置编码的类别？

追问：介绍下RoPE

RoPE有什么优缺点？

了解什么长文本外推方法？

DeepSeekR1 训练流程介绍
讲一讲moe的架构，如何保证专家负载均衡(负载不均衡解决思路)；qwen3的moe有共享专家吗
#### 门控网络是什么结构
softmax(mlp(x))
moe细粒度专家的好处
	
大模型对同一个输入两次推理的结果会一样吗？
介绍下大模型的超参数 (温度参数、top_k、top_p)
当前大模型存在哪些问题(回答了幻觉)
追问：如何减少幻觉出现的概率
大模型灾难性遗忘是什么？如何解决？
	
大模型有哪些技术设计让他能够处理更长的上下文
大模型分词器你了解哪些？(以BPE为例)简单介绍下原理
大模型词表怎么获得的(以BPE为例)
#### BPE和BBPE的区别
bpe 字符级，容易oov；bbpe 字节级，总共就256个，不会oov，BBPE考虑将一段文本的UTF-8编码(UTF-8保证任何语言都可以通用)中的一个字节256位不同的编码作为词表的初始化基础Subword。

LLaMA和QWEN最新模型分词器用的是什么？
大模型词表冗余是什么？有什么副作用？